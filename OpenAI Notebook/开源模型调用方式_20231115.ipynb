{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1921fc11",
   "metadata": {},
   "source": [
    "## Azure调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "#os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://omdbeta-ae-01.openai.azure.com\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701c2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"As an AI developed by OpenAI, I can't provide real-time data or updates. But here's a general way to find out the age of the current president of the USA:\\n\\n1. Firstly, you need to know who the current president is. As of my last update in October 2021, the president of the United States is Joe Biden.\\n\\n2. Joe Biden was born on November 20, 1942.\\n\\n3. To calculate his age, subtract the year he was born from the current year. If the current date is before November 20, subtract an additional year because his birthday hasn't occurred yet this year.\\n\\nRemember, for the most accurate and up-to-date information, you should look up the current president and their date of birth on a trusted source like a news website or an encyclopedia.\"\n"
     ]
    }
   ],
   "source": [
    "# 对话类langchain API调用\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4\")\n",
    "\n",
    "msg = HumanMessage(content=\"Explain step by step. How old is the president of USA?\")\n",
    "print(llm(messages=[msg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b3959",
   "metadata": {},
   "source": [
    "## 开源模型调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter,MarkdownTextSplitter\n",
    "import langchain.text_splitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import IPython\n",
    "import sentence_transformers\n",
    "from langchain.vectorstores import Chroma\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e90f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件夹\n",
    "qa_file = './qa.csv'\n",
    "qa_file_cleaned = './qa_clean.csv'\n",
    "embedding_file = './embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#问答对数据处理：一问一答处理成一个chunk\n",
    "df=pd.read_csv(qa_file)\n",
    "df['new'] = df['Similar questions'].str.strip().str.replace('\\n\\n', '')  + '\\n\\n'\n",
    "df['new'] \n",
    "df['new'].to_csv(qa_file_cleaned,encoding='utf-8-sig', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文档切分\n",
    "md_loader = TextLoader(qa_file_cleaned, encoding='utf8')\n",
    "md_doc = md_loader.load()\n",
    "\n",
    "markdown_splitter = CharacterTextSplitter(separator = '\\n\\n',  chunk_size=25, chunk_overlap=0 )\n",
    "md_docs = markdown_splitter.split_documents(md_doc)\n",
    "print(len(md_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 向量化知识库\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import IPython\n",
    "import sentence_transformers\n",
    "\n",
    "# 选择模型---https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads\n",
    "model_name = \"BAAI/bge-large-zh-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    #model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "embeddings.client = sentence_transformers.SentenceTransformer(embeddings.model_name) #, device='mps'\n",
    "print(\"embeddings\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地持久化向量库  \n",
    "db = Chroma.from_documents(md_docs, embeddings, persist_directory=embedding_file)\n",
    "\n",
    "# 加载本地持久化数据\n",
    "db = Chroma(persist_directory=embedding_file, embedding_function=embeddings)\n",
    "# print(dir(db))\n",
    "db.get(limit =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库索引参考文献\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def RetriveQA(question):\n",
    "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .7,\"k\": 3})\n",
    "    similarDocs = retriever.get_relevant_documents(question)\n",
    "    if len(similarDocs)>0:\n",
    "        similardocs1 = similarDocs[0].page_content.replace('\"', '').replace('\\n', '')\n",
    "        question1 = similardocs1\n",
    "        filtered_df = df_qa[df_qa['Similar questions']==str(question1)]\n",
    "        answer1 = filtered_df['Standard answer'].values\n",
    "        if len(similarDocs)>1:\n",
    "            similardocs2 = similarDocs[1].page_content.replace('\"', '').replace('\\n', '')\n",
    "            question2 = similardocs2\n",
    "            filtered_df = df_qa[df_qa['Similar questions']==str(question2)]\n",
    "            answer2 = filtered_df['Standard answer'].values\n",
    "            docs_input = \"【参考问题】\" + question1 + '\\n' + \"【参考答案】\" + answer1 + '\\n' + \"【参考问题】\" + question2 + '\\n' + \"【参考答案】\" + answer2 + '\\n'\n",
    "        else:\n",
    "            docs_input = \"【参考问题】\" + question1 + '\\n' + \"【参考答案】\" + answer1 + '\\n'\n",
    "    else:\n",
    "        docs_input = ''\n",
    "    return docs_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57a761",
   "metadata": {},
   "source": [
    "## openai调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954183fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "import math\n",
    "import os\n",
    "\n",
    "openai.api_key = \"***\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = '***'\n",
    "\n",
    "start_sequence = \"\\AI:\"\n",
    "restart_sequence = \"\\Human:\"\n",
    "\n",
    "##调用fine-tuning后的模型\n",
    "#ft_model = 'ada:ft-personal-2023-05-30-08-57-22'\n",
    "ft_model = 'ada:ft-personal-2023-06-20-07-48-30'\n",
    "\n",
    "prompt = \" \"\n",
    "\n",
    "##LABEL转换\n",
    "def testc(a):\n",
    "    if a == 0:\n",
    "        label = 'Booking'\n",
    "    elif a == 1:\n",
    "        label = 'Cancellation'\n",
    "    elif a == 2:\n",
    "        label = 'Carer'\n",
    "    elif a == 3:\n",
    "        label = 'Login'\n",
    "    elif a == 4:\n",
    "        label = 'Minor'\n",
    "    elif a == 5:\n",
    "        label = 'Others'\n",
    "    elif a == 6:\n",
    "        label = 'Patient Particulars'\n",
    "    elif a == 7:\n",
    "        label = 'Payment'\n",
    "    elif a == 8:\n",
    "        label = 'Record'\n",
    "    elif a == 9:\n",
    "        label = 'Registration'\n",
    "    elif a == 10:\n",
    "        label = 'Rescheduling'\n",
    "    else:\n",
    "        label = 'Others'\n",
    "    return label\n",
    "\n",
    "##获取分类及TOP3置信度\n",
    "def generate_response(prompt):\n",
    "    completion = openai.Completion.create(\n",
    "           model = ft_model,\n",
    "           prompt = prompt,\n",
    "           temperature = 0,\n",
    "           max_tokens= 1, \n",
    "           #top_p=1,\n",
    "           logprobs=5,\n",
    "           frequency_penalty=0, \n",
    "           presence_penalty=0\n",
    "       ) \n",
    "    #return completion['choices'][0]\n",
    "    top_p = None\n",
    "    df_op_new = None\n",
    "    try:\n",
    "        top_p = completion['choices'][0].text\n",
    "        top_p = testc(int(top_p))\n",
    "        output_prob = completion['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "        dict_op = output_prob.to_dict()\n",
    "        df_op = pd.DataFrame.from_dict(dict_op,orient='index',columns=['Confidence_log']).reset_index().rename(columns={'index':'Type'})\n",
    "        df_op['Confidence']=df_op['Confidence_log'].apply(lambda x: math.exp(x))\n",
    "        #df_op['LABEL'] = df_op['Type'].apply(lambda x: testc(x))\n",
    "        df_op['Type_adj']=df_op['Type'].astype('int')##空格问题\n",
    "        df_op_new = df_op.groupby('Type_adj').sum().reset_index().sort_values(by='Confidence',ascending=False).iloc[:3,:]\n",
    "        df_op_new['LABEL'] = df_op_new['Type_adj'].apply(lambda x: testc(x))\n",
    "        \n",
    "    except:\n",
    "        top_p = None\n",
    "        df_op_new = None\n",
    "    return top_p, df_op_new\n",
    "\n",
    "\n",
    "##主函数\n",
    "def my_chatbot(input, history):\n",
    "    #history = history or []\n",
    "    history = []\n",
    "    my_history = list(sum(history, ()))\n",
    "    my_history.append(input)\n",
    "    my_input = ' '.join(my_history)\n",
    "    my_input = my_input+'\\n\\n###\\n\\n'\n",
    "    my_input.replace(' two', '2')\n",
    "    output1,output2 = generate_response(my_input)\n",
    "    #history.append((input, output))\n",
    "    return output1,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 不需运行\n",
    "## 仅首次构建本地向量库需运行，其他时候不需要\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI,VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from getpass import getpass\n",
    "import os\n",
    "import openai\n",
    "import gradio as gr\n",
    "import math\n",
    "\n",
    "openai.api_key = \"***\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = '***'\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 添加预训练文档库\n",
    "def add_documents(loader, instance):\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separators= [\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"])\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    instance.add_documents(texts)\n",
    "\n",
    "\n",
    "# 创建Chroma实例\n",
    "instance = Chroma(embedding_function=embeddings, persist_directory=\"./DataProcessing/embedding_index\")\n",
    "\n",
    "# 添加本地知识库 (CSV file)，HA-GO常见问题\n",
    "loader = TextLoader('./常見問題.csv')\n",
    "add_documents(loader, instance)\n",
    "\n",
    "# 持久化向量库\n",
    "instance.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request,make_response\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#初始化向量库，供本地调用使用\n",
    "instance = Chroma(persist_directory=\"./DataProcessing/embedding_index\", embedding_function=embeddings)\n",
    "\n",
    "#prompt模版\n",
    "tech_template = \"\"\"你是一位医院APP助手，请根据提供的知识库学习【Question】以及对应的【Answer】，找出最相近的【Question】并只输出其对应的【Answer】\n",
    "\n",
    "注意：只输出对应的【Answer】即可，不需要任何其他的内容，如果不确定，就请返回\"9999\"\n",
    "\n",
    "{summaries}\n",
    "Q: {question}\n",
    "A: \"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=tech_template, input_variables=[\"summaries\",\"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "                                                chain_type=\"stuff\",\n",
    "                                                #retriever=instance.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":1}),\n",
    "                                                retriever=instance.as_retriever(),\n",
    "                                                chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                                reduce_k_below_max_tokens=True\n",
    "                                )\n",
    "\n",
    "\n",
    "def getAnswer(index):\n",
    "    try:\n",
    "        index = int(index)\n",
    "        #releQuestion = qa_answer.loc[qa_answer['Index']==index,(['Answer'])]\n",
    "        result = qa_answer.loc[qa_answer['Index']==index,(['Question','Answer'])]\n",
    "    except:\n",
    "        result = qa_answer.loc[qa_answer['Question'].str.contains(index),(['Question','Answer'])]\n",
    "    #result = qa_answer.loc[qa_answer['Index']==index,(['Answer'])]\n",
    "    #result = qa_answer.loc[(qa_answer['Index']==int(index)) | (qa_answer['Question'].str.contains(result['answer'])),(['Answer'])]\n",
    "    return result\n",
    "\n",
    "def qa_hago(query):\n",
    "    result = qa({\"question\": query}, return_only_outputs=True)\n",
    "    index = result['answer']\n",
    "    try:\n",
    "        result = getAnswer(index)\n",
    "        #result = result.values[0]\n",
    "        ### 把问题也提取出来\n",
    "        #result = str(result)[1:-1]\n",
    "        qs = 'Most Relevant Question: ' + str(result['Question'].values[0]) + '\\n'\n",
    "        ans = 'Answer: ' + str(result['Answer'].values[0])\n",
    "        res = qs + ans\n",
    "    except:\n",
    "        res = '我是一个基于大语言模型的医疗智能助手，抱歉这个问题在现有知识库中无法找到相关答案，请您联系人工服务，谢谢您，祝您快乐'\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "blocks = gr.Blocks()\n",
    "\n",
    "with blocks as demo: \n",
    "    gr.Markdown(\"\"\"<h1><center>Complaints Classification for HA</center></h1>\"\"\")\n",
    "    gr.Markdown(\"\"\"<h3><center>Produced by Tencent Healthcare</center></h3>\"\"\")\n",
    "    \n",
    "    state = gr.State()\n",
    "    \n",
    "    with gr.Row(scale=3,min_width=600):\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"请输入用户投诉内容，我可以帮你进行分类\",lines=5).style(container=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        btn = gr.Button(\"输出类别\")\n",
    "        reverse_btn = gr.Button(\"输出置信度\")\n",
    "        \n",
    "    output1 = gr.Textbox(label=\"判定类别：\")\n",
    "    output2 = gr.Textbox(value = '',label=\"TOP3类别置信度：\",lines=4)\n",
    "    \n",
    "    def testa(txt,state):\n",
    "        a,b = my_chatbot(txt,state)\n",
    "        return a\n",
    "    \n",
    "    def testb(txt,state):\n",
    "        a,b = my_chatbot(txt,state)\n",
    "        return b\n",
    "    \n",
    "\n",
    "    \n",
    "    btn.click(testa, inputs=[txt,state], outputs=output1)\n",
    "    reverse_btn.click(testb, [txt,state], outputs=output2)\n",
    "    #reverse_btn.click(my_chatbot, [txt], output2, _js=\"(s, v, o) => o + ' ' + v + ' ' + s\")\n",
    "    \n",
    "    btn_plot = gr.Button(\"输出柱状图\")\n",
    "    #inp = inputs.Textbox()\n",
    "    #df_op = testb(txt,state)\n",
    "    df_op = txt.submit(testb, inputs=[txt, state], outputs=None)\n",
    "    output3 = gr.BarPlot(value=df_op,x='LABEL',y='Confidence', vertical=False,title='类别置信度')\n",
    "    btn_plot.click(testb, [txt,state], output3)\n",
    "    #output4 = gr.Interface(None, None, None)\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
